{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-n2e0BkhEKS"
   },
   "source": [
    "# **Homework 10 - Adversarial Attack**\n",
    "\n",
    "Slides: https://reurl.cc/7DDxnD\n",
    "\n",
    "Contact: ntu-ml-2022spring-ta@googlegroups.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RX7iRXrhMA_"
   },
   "source": [
    "## Enviroment & Download\n",
    "\n",
    "We make use of [pytorchcv](https://pypi.org/project/pytorchcv/) to obtain CIFAR-10 pretrained model, so we need to set up the enviroment first. We also need to download the data (200 images) which we want to attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4Lw7urignqP",
    "outputId": "c5507e2a-759e-4bd7-c5f2-754f08e4cce6"
   },
   "outputs": [],
   "source": [
    "# set up environment\n",
    "!pip install pytorchcv\n",
    "!pip install imgaug\n",
    "!pip install torchattacks\n",
    "\n",
    "# download\n",
    "!wget https://github.com/DanielLin94144/ML-attack-dataset/files/8167812/data.zip\n",
    "\n",
    "# unzip\n",
    "!unzip ./data.zip\n",
    "!rm ./data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AVdxFvfb1qL",
    "outputId": "14c13fca-19ef-453b-8f41-faa58eb80cef"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5inbFx_alYjw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda', 0)\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkQQf0l1hbBs"
   },
   "source": [
    "## Global Settings \n",
    "#### **[NOTE]**: Don't change the settings here, or your generated image might not meet the constraint.\n",
    "* $\\epsilon$ is fixed to be 8. But on **Data section**, we will first apply transforms on raw pixel value (0-255 scale) **by ToTensor (to 0-1 scale)** and then **Normalize (subtract mean divide std)**. $\\epsilon$ should be set to $\\frac{8}{255 * std}$ during attack.\n",
    "\n",
    "* Explaination (optional)\n",
    "    * Denote the first pixel of original image as $p$, and the first pixel of adversarial image as $a$.\n",
    "    * The $\\epsilon$ constraints tell us $\\left| p-a \\right| <= 8$.\n",
    "    * ToTensor() can be seen as a function where $T(x) = x/255$.\n",
    "    * Normalize() can be seen as a function where $N(x) = (x-mean)/std$ where $mean$ and $std$ are constants.\n",
    "    * After applying ToTensor() and Normalize() on $p$ and $a$, the constraint becomes $\\left| N(T(p))-N(T(a)) \\right| = \\left| \\frac{\\frac{p}{255}-mean}{std}-\\frac{\\frac{a}{255}-mean}{std} \\right| = \\frac{1}{255 * std} \\left| p-a \\right| <= \\frac{8}{255 * std}.$\n",
    "    * So, we should set $\\epsilon$ to be $\\frac{8}{255 * std}$ after ToTensor() and Normalize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACghc_tsg2vE"
   },
   "outputs": [],
   "source": [
    "# the mean and std are the calculated statistics from cifar_10 dataset\n",
    "cifar_10_mean = (0.491, 0.482, 0.447) # mean for the three channels of cifar_10 images\n",
    "cifar_10_std = (0.202, 0.199, 0.201) # std for the three channels of cifar_10 images\n",
    "\n",
    "# convert mean and std to 3-dimensional tensors for future operations\n",
    "mean = torch.tensor(cifar_10_mean).to(device).view(3, 1, 1)\n",
    "std = torch.tensor(cifar_10_std).to(device).view(3, 1, 1)\n",
    "\n",
    "epsilon = 8/255/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uO8f0NmtlM63"
   },
   "outputs": [],
   "source": [
    "root = './data' # directory for storing benign images\n",
    "# benign images: images which do not contain adversarial perturbations\n",
    "# adversarial images: images which include adversarial perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhBJBAlKherZ"
   },
   "source": [
    "## Data\n",
    "\n",
    "Construct dataset and dataloader from root directory. Note that we store the filename of each image for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnrVtLmAdS6q"
   },
   "outputs": [],
   "source": [
    "# import torchattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfUkvnibdVFG"
   },
   "outputs": [],
   "source": [
    "# # alpha and num_iter can be decided by yourself\n",
    "# alpha = 0.8/255/std\n",
    "# def pgd(model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=20):\n",
    "#     atk = torchattacks.PGD(model, eps=epsilon, alpha=alpha, steps=num_iter)\n",
    "#     adv_images = atk(x, y)\n",
    "#     adv_images = torch.max(torch.min(adv_images, x+epsilon), x-epsilon) # clip new x_adv back to [x-epsilon, x+epsilon]\n",
    "#     return adv_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXpRAHz0hkDt",
    "outputId": "3f6ae8da-e077-493d-99a0-709bab9b47e6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar_10_mean, cifar_10_std)\n",
    "])\n",
    "\n",
    "class AdvDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.names = []\n",
    "        '''\n",
    "        data_dir\n",
    "        ├── class_dir\n",
    "        │   ├── class1.png\n",
    "        │   ├── ...\n",
    "        │   ├── class20.png\n",
    "        '''\n",
    "        for i, class_dir in enumerate(sorted(glob.glob(f'{data_dir}/*'))):\n",
    "            images = sorted(glob.glob(f'{class_dir}/*'))\n",
    "            self.images += images\n",
    "            self.labels += ([i] * len(images))\n",
    "            self.names += [os.path.relpath(imgs, data_dir) for imgs in images]\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.transform(Image.open(self.images[idx]))\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "    def __getname__(self):\n",
    "        return self.names\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "adv_set = AdvDataset(root, transform=transform)\n",
    "adv_names = adv_set.__getname__()\n",
    "adv_loader = DataLoader(adv_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'number of images = {adv_set.__len__()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW2rYb7pX6Z4"
   },
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    # random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnszlTsYrTQZ"
   },
   "source": [
    "## Utils -- Benign Images Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c_zZLzkrceE"
   },
   "outputs": [],
   "source": [
    "# to evaluate the performance of model on benign images\n",
    "def epoch_benign(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    train_acc, train_loss = 0.0, 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        yp = model(x)\n",
    "        loss = loss_fn(yp, y)\n",
    "        train_acc += (yp.argmax(dim=1) == y).sum().item()\n",
    "        train_loss += loss.item() * x.shape[0]\n",
    "    return train_acc / len(loader.dataset), train_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YJxK7YehqQy"
   },
   "source": [
    "## Utils -- Attack Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_1wKfKyhrQW"
   },
   "outputs": [],
   "source": [
    "# perform fgsm attack\n",
    "def fgsm(model, x, y, loss_fn, epsilon=epsilon):\n",
    "    x_adv = x.detach().clone() # initialize x_adv as original benign image x\n",
    "    x_adv.requires_grad = True # need to obtain gradient of x_adv, thus set required grad\n",
    "    loss = loss_fn(model(x_adv), y) # calculate loss\n",
    "    loss.backward() # calculate gradient\n",
    "    # fgsm: use gradient ascent on x_adv to maximize loss\n",
    "    grad = x_adv.grad.detach()\n",
    "    x_adv = x_adv + epsilon * grad.sign()\n",
    "    return x_adv\n",
    "\n",
    "# alpha and num_iter can be decided by yourself\n",
    "alpha = 0.8/255/std\n",
    "def ifgsm(model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=20):\n",
    "    x_adv = x\n",
    "    # write a loop of num_iter to represent the iterative times\n",
    "    for i in range(num_iter):\n",
    "        # x_adv = fgsm(model, x_adv, y, loss_fn, alpha) # call fgsm with (epsilon = alpha) to obtain new x_adv\n",
    "        x_adv = x_adv.detach().clone()\n",
    "        x_adv.requires_grad = True # need to obtain gradient of x_adv, thus set required grad\n",
    "        loss = loss_fn(model(x_adv), y) # calculate loss\n",
    "        loss.backward() # calculate gradient\n",
    "        # fgsm: use gradient ascent on x_adv to maximize loss\n",
    "        grad = x_adv.grad.detach()\n",
    "        x_adv = x_adv + alpha * grad.sign()\n",
    "\n",
    "        x_adv = torch.max(torch.min(x_adv, x+epsilon), x-epsilon) # clip new x_adv back to [x-epsilon, x+epsilon]\n",
    "    return x_adv\n",
    "\n",
    "\n",
    "# MIFGSM 參考以下連結撰寫\n",
    "# https://github.com/Harry24k/adversarial-attacks-pytorch/blob/master/torchattacks/attacks/mifgsm.py\n",
    "\n",
    "def mifgsm(model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=20, decay=1.0):\n",
    "    x_adv = x\n",
    "    # initialze momentum tensor\n",
    "    momentum = torch.zeros_like(x).detach().to(device)\n",
    "    # write a loop of num_iter to represent the iterative times\n",
    "    for i in range(num_iter):\n",
    "        x_adv = x_adv.detach().clone()\n",
    "        x_adv.requires_grad = True # need to obtain gradient of x_adv, thus set required grad\n",
    "        loss = loss_fn(model(x_adv), y) # calculate loss\n",
    "        loss.backward() # calculate gradient\n",
    "        # TODO: Momentum calculation\n",
    "        # Update adversarial images\n",
    "        # grad = .....\n",
    "        # grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "        grad = x_adv.grad.detach()\n",
    "        grad = grad / torch.mean(torch.abs(grad), dim=(1,2,3), keepdim=True)\n",
    "        grad = grad + momentum * decay\n",
    "        momentum = grad\n",
    "        x_adv = x_adv + alpha * grad.sign()\n",
    "        x_adv = torch.max(torch.min(x_adv, x+epsilon), x-epsilon) # clip new x_adv back to [x-epsilon, x+epsilon]\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRJR2iW6lo3X"
   },
   "outputs": [],
   "source": [
    "# DIM-MIFGSM 參考以下連結撰寫\n",
    "# https://github.com/Harry24k/adversarial-attacks-pytorch/blob/master/torchattacks/attacks/difgsm.py\n",
    "\n",
    "def input_diversity(x, resize_rate, diversity_prob):\n",
    "    img_size = x.shape[-1]\n",
    "    img_resize = int(img_size * resize_rate)\n",
    "\n",
    "    if resize_rate < 1:\n",
    "        img_size = img_resize\n",
    "        img_resize = x.shape[-1]\n",
    "\n",
    "    rnd = torch.randint(low=img_size, high=img_resize, size=(1,), dtype=torch.int32)\n",
    "    rescaled = F.interpolate(x, size=[rnd, rnd], mode='bilinear', align_corners=False)\n",
    "    h_rem = img_resize - rnd\n",
    "    w_rem = img_resize - rnd\n",
    "    pad_top = torch.randint(low=0, high=h_rem.item(), size=(1,), dtype=torch.int32)\n",
    "    pad_bottom = h_rem - pad_top\n",
    "    pad_left = torch.randint(low=0, high=w_rem.item(), size=(1,), dtype=torch.int32)\n",
    "    pad_right = w_rem - pad_left\n",
    "\n",
    "    padded = F.pad(rescaled, [pad_left.item(), pad_right.item(), pad_top.item(), pad_bottom.item()], value=0)\n",
    "\n",
    "    return padded if torch.rand(1) < diversity_prob else x\n",
    "\n",
    "# num_iter=20 -> 40\n",
    "def dmifgsm(model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=40, decay=1.0,\n",
    "            resize_rate=0.9, diversity_prob=0.5):\n",
    "    x_adv = x\n",
    "    # initialze momentum tensor\n",
    "    momentum = torch.zeros_like(x).detach().to(device)\n",
    "\n",
    "    # write a loop of num_iter to represent the iterative times\n",
    "    for i in range(num_iter):\n",
    "        x_adv = x_adv.detach().clone()\n",
    "        x_adv.requires_grad = True # need to obtain gradient of x_adv, thus set required grad\n",
    "        loss = loss_fn(model(input_diversity(x_adv, resize_rate, diversity_prob)), y) # calculate loss\n",
    "        loss.backward() # calculate gradient\n",
    "        # TODO: Momentum calculation\n",
    "        # Update adversarial images\n",
    "        # grad = .....\n",
    "        # grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "        grad = x_adv.grad.detach()\n",
    "        grad = grad / torch.mean(torch.abs(grad), dim=(1,2,3), keepdim=True)\n",
    "        grad = grad + momentum * decay\n",
    "        momentum = grad\n",
    "        x_adv = x_adv + alpha * grad.sign()\n",
    "        x_adv = torch.max(torch.min(x_adv, x+epsilon), x-epsilon) # clip new x_adv back to [x-epsilon, x+epsilon]\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYCEQwmcrmH6"
   },
   "source": [
    "## Utils -- Attack\n",
    "* Recall\n",
    "  * ToTensor() can be seen as a function where $T(x) = x/255$.\n",
    "  * Normalize() can be seen as a function where $N(x) = (x-mean)/std$ where $mean$ and $std$ are constants.\n",
    "\n",
    "* Inverse function\n",
    "  * Inverse Normalize() can be seen as a function where $N^{-1}(x) = x*std+mean$ where $mean$ and $std$ are constants.\n",
    "  * Inverse ToTensor() can be seen as a function where $T^{-1}(x) = x*255$.\n",
    "\n",
    "* Special Noted\n",
    "  * ToTensor() will also convert the image from shape (height, width, channel) to shape (channel, height, width), so we also need to transpose the shape back to original shape.\n",
    "  * Since our dataloader samples a batch of data, what we need here is to transpose **(batch_size, channel, height, width)** back to **(batch_size, height, width, channel)** using np.transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5X_9x-7ro_w"
   },
   "outputs": [],
   "source": [
    "# perform adversarial attack and generate adversarial examples\n",
    "def gen_adv_examples(model, loader, attack, loss_fn):\n",
    "    model.eval()\n",
    "    adv_names = []\n",
    "    train_acc, train_loss = 0.0, 0.0\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x_adv = attack(model, x, y, loss_fn) # obtain adversarial examples\n",
    "        yp = model(x_adv)\n",
    "        loss = loss_fn(yp, y)\n",
    "        train_acc += (yp.argmax(dim=1) == y).sum().item()\n",
    "        train_loss += loss.item() * x.shape[0]\n",
    "        # store adversarial examples\n",
    "        adv_ex = ((x_adv) * std + mean).clamp(0, 1) # to 0-1 scale\n",
    "        adv_ex = (adv_ex * 255).clamp(0, 255) # 0-255 scale\n",
    "        adv_ex = adv_ex.detach().cpu().data.numpy().round() # round to remove decimal part\n",
    "        adv_ex = adv_ex.transpose((0, 2, 3, 1)) # transpose (bs, C, H, W) back to (bs, H, W, C)\n",
    "        adv_examples = adv_ex if i == 0 else np.r_[adv_examples, adv_ex]\n",
    "    return adv_examples, train_acc / len(loader.dataset), train_loss / len(loader.dataset)\n",
    "\n",
    "# create directory which stores adversarial examples\n",
    "def create_dir(data_dir, adv_dir, adv_examples, adv_names):\n",
    "    if os.path.exists(adv_dir) is not True:\n",
    "        _ = shutil.copytree(data_dir, adv_dir)\n",
    "    for example, name in zip(adv_examples, adv_names):\n",
    "        im = Image.fromarray(example.astype(np.uint8)) # image pixel value should be unsigned int\n",
    "        im.save(os.path.join(adv_dir, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_pMkmPytX3k"
   },
   "source": [
    "## Model / Loss Function\n",
    "\n",
    "Model list is available [here](https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/model_provider.py). Please select models which has _cifar10 suffix. Some of the models cannot be accessed/loaded. You can safely skip them since TA's model will not use those kinds of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchcv.model_provider import get_model as ptcv_get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwto8xbPtYzQ",
    "outputId": "44598c96-3aaf-4801-d59d-c487b6cbd375"
   },
   "outputs": [],
   "source": [
    "# model = ptcv_get_model('resnet110_cifar10', pretrained=True).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# benign_acc, benign_loss = epoch_benign(model, adv_loader, loss_fn)\n",
    "# print(f'benign_acc = {benign_acc:.5f}, benign_loss = {benign_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uslb7GPchtMI"
   },
   "source": [
    "## FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQwPTVUIhuTS",
    "outputId": "63943e0a-b1f6-4e7f-cff6-344fbd1347d7"
   },
   "outputs": [],
   "source": [
    "# adv_examples, fgsm_acc, fgsm_loss = gen_adv_examples(model, adv_loader, fgsm, loss_fn)\n",
    "# print(f'fgsm_acc = {fgsm_acc:.5f}, fgsm_loss = {fgsm_loss:.5f}')\n",
    "\n",
    "# create_dir(root, 'fgsm', adv_examples, adv_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXw6p0A6shZm"
   },
   "source": [
    "## I-FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUEsT06Iskt2"
   },
   "outputs": [],
   "source": [
    "# adv_examples, ifgsm_acc, ifgsm_loss = gen_adv_examples(model, adv_loader, ifgsm, loss_fn)\n",
    "# print(f'ifgsm_acc = {ifgsm_acc:.5f}, ifgsm_loss = {ifgsm_loss:.5f}')\n",
    "\n",
    "# create_dir(root, 'ifgsm', adv_examples, adv_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQ-nYkkYexEE"
   },
   "source": [
    "## Compress the images\n",
    "* Submit the .tgz file to [JudgeBoi](https://ml.ee.ntu.edu.tw/hw10/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItRo_S0M264N",
    "outputId": "253a59b1-f12b-48d6-933d-0557e06e6ce5"
   },
   "outputs": [],
   "source": [
    "# %cd fgsm\n",
    "# !tar zcvf ../fgsm.tgz *\n",
    "# %cd ..\n",
    "\n",
    "# # %cd ifgsm\n",
    "# # !tar zcvf ../ifgsm.tgz *\n",
    "# # %cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLZLbebigCA2"
   },
   "source": [
    "## Example of Ensemble Attack\n",
    "* Ensemble multiple models as your proxy model to increase the black-box transferability ([paper](https://arxiv.org/abs/1611.02770))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJcKiQNUgnPQ"
   },
   "outputs": [],
   "source": [
    "class ensembleNet(nn.Module):\n",
    "    def __init__(self, model_names):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([ptcv_get_model(name, pretrained=True) for name in model_names])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ensemble_logits = 0\n",
    "        for i, m in enumerate(self.models):\n",
    "            # TODO: sum up logits from multiple models  \n",
    "            # return ensemble_logits\n",
    "            logits = m(x.clone())\n",
    "            ensemble_logits += logits\n",
    "        ensemble_logits = ensemble_logits / len(self.models)\n",
    "        return ensemble_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjfJwJKeeaR2"
   },
   "source": [
    "* Construct your ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stYFytogeIzI",
    "outputId": "93f22b54-5f63-405c-9093-0a091488ed41"
   },
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'resnet20_cifar10', #\n",
    "    'resnet56_cifar10', #\n",
    "    'preresnet20_cifar10', #\n",
    "    'preresnet56_cifar10', #\n",
    "    'seresnet20_cifar10', #\n",
    "    'seresnet56_cifar10', #\n",
    "    'sepreresnet20_cifar10',\n",
    "    'sepreresnet56_cifar10',\n",
    "    'diaresnet20_cifar10',\n",
    "    'diaresnet56_cifar10',\n",
    "    'diapreresnet20_cifar10', #\n",
    "    'diapreresnet56_cifar10' #\n",
    "]\n",
    "\n",
    "# boss baseline: 0.04\n",
    "\n",
    "ensemble_model = ensembleNet(model_names).to(device)\n",
    "ensemble_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKgCicYKb1qf"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "benign_acc, benign_loss = epoch_benign(ensemble_model, adv_loader, loss_fn)\n",
    "print(f'benign_acc = {benign_acc:.5f}, benign_loss = {benign_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7QEzYa9e7l9"
   },
   "outputs": [],
   "source": [
    "ATTACK_METHOD_STR = \"dmifgsm\"\n",
    "ATTACK_METHOD = dmifgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7XkRZJhb1qg"
   },
   "outputs": [],
   "source": [
    "adv_examples, ifgsm_acc, ifgsm_loss = gen_adv_examples(ensemble_model, adv_loader, ATTACK_METHOD, loss_fn)\n",
    "print(f'{ATTACK_METHOD_STR}_acc = {ifgsm_acc:.5f}, {ATTACK_METHOD_STR}_loss = {ifgsm_loss:.5f}')\n",
    "\n",
    "create_dir(root, ATTACK_METHOD_STR, adv_examples, adv_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TFd-1bzr6LU"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJntCIrKb1qh"
   },
   "outputs": [],
   "source": [
    "%cd dmifgsm\n",
    "!tar zcvf ../dmifgsm.tgz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FM_S886kFd8"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "2FCuE2njkH1O",
    "outputId": "6ceb82e6-f4a3-4ffd-ee62-f7dc8c1223ae"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "cnt = 0\n",
    "for i, cls_name in enumerate(classes):\n",
    "    path = f'{cls_name}/{cls_name}1.png'\n",
    "    # benign image\n",
    "    cnt += 1\n",
    "    plt.subplot(len(classes), 4, cnt)\n",
    "    im = Image.open(f'./data/{path}')\n",
    "    logit = ensemble_model(transform(im).unsqueeze(0).to(device))[0]\n",
    "    predict = logit.argmax(-1).item()\n",
    "    prob = logit.softmax(-1)[predict].item()\n",
    "    plt.title(f'benign: {cls_name}1.png\\n{classes[predict]}: {prob:.2%}')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.array(im))\n",
    "    # adversarial image\n",
    "    cnt += 1\n",
    "    plt.subplot(len(classes), 4, cnt)\n",
    "    im = Image.open(f'./{ATTACK_METHOD_STR}/{path}')\n",
    "    logit = ensemble_model(transform(im).unsqueeze(0).to(device))[0]\n",
    "    predict = logit.argmax(-1).item()\n",
    "    prob = logit.softmax(-1)[predict].item()\n",
    "    plt.title(f'adversarial: {cls_name}1.png\\n{classes[predict]}: {prob:.2%}')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.array(im))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUmKa02Vmp29"
   },
   "source": [
    "## Report Question\n",
    "* Make sure you follow below setup: the source model is \"resnet110_cifar10\", applying the vanilla fgsm attack on `dog2.png`. You can find the perturbed image in `fgsm/dog2.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ptcv_get_model('resnet110_cifar10', pretrained=True).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "benign_acc, benign_loss = epoch_benign(model, adv_loader, loss_fn)\n",
    "print(f'benign_acc = {benign_acc:.5f}, benign_loss = {benign_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_examples, fgsm_acc, fgsm_loss = gen_adv_examples(model, adv_loader, fgsm, loss_fn)\n",
    "print(f'fgsm_acc = {fgsm_acc:.5f}, fgsm_loss = {fgsm_loss:.5f}')\n",
    "\n",
    "create_dir(root, 'fgsm', adv_examples, adv_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "8NW8ntCKY3VY",
    "outputId": "c3e1340b-36dc-4a1d-d9f5-34b3459fe5e4"
   },
   "outputs": [],
   "source": [
    "# original image\n",
    "path = f'dog/dog2.png'\n",
    "im = Image.open(f'./data/{path}')\n",
    "logit = model(transform(im).unsqueeze(0).to(device))[0]\n",
    "predict = logit.argmax(-1).item()\n",
    "prob = logit.softmax(-1)[predict].item()\n",
    "plt.title(f'benign: dog2.png\\n{classes[predict]}: {prob:.2%}')\n",
    "plt.axis('off')\n",
    "plt.imshow(np.array(im))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# adversarial image \n",
    "adv_im = Image.open(f'./fgsm/{path}')\n",
    "logit = model(transform(adv_im).unsqueeze(0).to(device))[0]\n",
    "predict = logit.argmax(-1).item()\n",
    "prob = logit.softmax(-1)[predict].item()\n",
    "plt.title(f'adversarial: dog2.png\\n{classes[predict]}: {prob:.2%}')\n",
    "plt.axis('off')\n",
    "plt.imshow(np.array(adv_im))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AQkofrTnePa"
   },
   "source": [
    "## Passive Defense - JPEG compression\n",
    "JPEG compression by imgaug package, compression rate set to 70\n",
    "\n",
    "Reference: https://imgaug.readthedocs.io/en/latest/source/api_augmenters_arithmetic.html#imgaug.augmenters.arithmetic.JpegCompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "sKuQaPp2mz7C",
    "outputId": "ece7cee3-8229-48ed-fc33-75bee28e3769"
   },
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "\n",
    "# pre-process image\n",
    "x = transforms.ToTensor()(adv_im)*255\n",
    "x = x.permute(1, 2, 0).numpy()\n",
    "x = x.astype(np.uint8)\n",
    "\n",
    "# TODO: use \"imgaug\" package to perform JPEG compression (compression rate = 70)\n",
    "# compressed_x =  ... x .. \n",
    "aug = iaa.JpegCompression(compression=70)\n",
    "compressed_x = aug.augment_image(x)\n",
    "\n",
    "logit = model(transform(compressed_x).unsqueeze(0).to(device))[0]\n",
    "predict = logit.argmax(-1).item()\n",
    "prob = logit.softmax(-1)[predict].item()\n",
    "plt.title(f'JPEG adversarial: dog2.png\\n{classes[predict]}: {prob:.2%}')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "plt.imshow(compressed_x)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML2022_hw10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kuokuo_env",
   "language": "python",
   "name": "kuokuo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
