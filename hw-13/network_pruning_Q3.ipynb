{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful packages for this homework\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset # \"ConcatDataset\" and \"Subset\" are possibly useful\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# define testing transforms\n",
    "test_tfm = transforms.Compose([\n",
    "    # It is not encouraged to modify this part if you are using the provided teacher model. This transform is stardard and good enough for testing.\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, path, tfm=test_tfm, files = None):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
    "        if files != None:\n",
    "            self.files = files\n",
    "        print(f\"One {path} sample\",self.files[0])\n",
    "        self.transform = tfm\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.files[idx]\n",
    "        im = Image.open(fname)\n",
    "        im = self.transform(im)\n",
    "        try:\n",
    "            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n",
    "        except:\n",
    "            label = -1 # test has no label\n",
    "        return im,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form valid dataloaders\n",
    "valid_set = FoodDataset(os.path.join('./food11-hw13', \"validation\"), tfm=test_tfm)\n",
    "valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    valid_accs = []\n",
    "    valid_lens = []\n",
    "\n",
    "    for batch in tqdm(valid_loader):\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # We don't need gradient in validation.\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "            logits = model(imgs) # MEDIUM BASELINE\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        batch_len = len(imgs)\n",
    "        valid_accs.append(acc)\n",
    "        valid_lens.append(batch_len)\n",
    "\n",
    "    # The average accuracy for entire validation set is the average of the recorded values.\n",
    "    valid_acc = sum(valid_accs) / sum(valid_lens)\n",
    "    return valid_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWEnrhWatOQb"
   },
   "source": [
    "Let's say now you want to prune all the parameters named with `weight` in all the `nn.Conv2d` layers in the `model`, with pruning ratio **0.2**. Then please refer to the code below to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc_list = []\n",
    "\n",
    "for ratio in np.arange(0, 1, 0.05):\n",
    "    # Specify the pruning ratio\n",
    "    ratio = round(ratio, 2)\n",
    "    # Load model\n",
    "    teacher_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=11)\n",
    "    teacher_ckpt_path = os.path.join('./food11-hw13', \"resnet18_teacher.ckpt\")\n",
    "    teacher_model.load_state_dict(torch.load(teacher_ckpt_path, map_location='cpu'))\n",
    "    for name, module in teacher_model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d): # if the nn.module is torch.nn.Conv2d\n",
    "            prune.l1_unstructured(module, name='weight', amount=ratio) # use 'prune' method provided by 'torch.nn.utils.prune' to prune the weight parameters in the nn.Conv2d layers\n",
    "    # Next, you just have to generize the above code to different ratio and test the accuracy on the validation set of food11-hw13.\n",
    "    valid_acc = evaluate(teacher_model)\n",
    "    valid_acc_list.append(valid_acc)\n",
    "    print(valid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(np.arange(0, 1, 0.05), valid_acc_list, \"-o\")\n",
    "plt.grid(ls=\"--\")\n",
    "plt.xticks(np.arange(0, 1, 0.05))\n",
    "plt.title(\"Pruning Ratio vs. Model Accuracy\")\n",
    "plt.xlabel(\"Pruning Ratio\")\n",
    "plt.ylabel(\"Model Accuracy\")\n",
    "# plt.savefig(\"pruning.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc_list"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "network pruning example for report Q3-1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kuokuo_env",
   "language": "python",
   "name": "kuokuo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
