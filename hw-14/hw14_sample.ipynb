{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgJbeG8EGMCL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vzj98NRfoAd4"
   },
   "source": [
    "\n",
    "\n",
    "### LifeLong Machine Learning\n",
    "### TA's Slide\n",
    "[Slide](https://docs.google.com/presentation/d/1SMJLWPTPCIrZdNdAjrS4zQZx1kfB73jCFSb7JRX90gQ/edit?usp=sharing)\n",
    "\n",
    "### Definition\n",
    "The detailed explanations and definitions of LifeLong Learning please refer to [LifeLong learning](https://youtu.be/7qT5P9KJnWo) \n",
    "\n",
    "\n",
    "### Methods\n",
    "Someone proposed a survey paper for LifeLong Learning at the end of 2019 to distinguish 2016-2019 LigeLong Learning methods into three families.\n",
    "\n",
    "We can distinguish LifeLong Learning methods into three families, based on how task\n",
    "specific information is stored and used throughout the sequential learning process:\n",
    "* Replay-based methods\n",
    "* Regularization-based methods\n",
    "* Parameter isolation methods\n",
    "\n",
    "<img src=\"https://i.ibb.co/VDFJkWG/2019-12-29-17-25.png\" width=\"100%\">\n",
    "\n",
    "In this assignment, we have to go through EWC, MAS, SI, Remanian Walk, SCP Methods in the prior-focused methods of the regularization-based methods. \n",
    "\n",
    "Source: [Continual Learning in Neural\n",
    "Networks](https://arxiv.org/pdf/1910.02718.pdf)\n",
    "\n",
    "Please feel free to mail us if you have any questions.\n",
    "\n",
    "ntu-ml-2022spring-ta@googlegroups.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdEFq3wqsCHB"
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4Y8VK-lfnA0"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbCbsCR0r1rQ"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUz2RKk-ftcK"
   },
   "source": [
    "### Check devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZbpbUMINUBk",
    "outputId": "b4fe903a-00de-4b9d-a03c-0f7033d0cfb5"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZjNHdafsPlm"
   },
   "source": [
    "### Fix Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MX0TGy3iriy8"
   },
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "  np.random.seed(seed)  \n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-R-ocGK8sf_f"
   },
   "source": [
    "# Prepare Data\n",
    "We utilize rotated MNIST as our training dataset.\n",
    "\n",
    "So, first, we utilize 5 different rotations to generate 10 different rotated MNISTs as different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvWzrg9qsrOr"
   },
   "source": [
    "### Rotation and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKzqghAlnvtN"
   },
   "outputs": [],
   "source": [
    "# Rotate MNIST to generate 10 tasks\n",
    "\n",
    "def _rotate_image(image, angle):\n",
    "  if angle is None:\n",
    "    return image\n",
    "\n",
    "  image = transforms.functional.rotate(image, angle=angle)\n",
    "  return image\n",
    "\n",
    "def get_transform(angle=None):\n",
    "  transform = transforms.Compose([transforms.ToTensor(),\n",
    "                   transforms.Lambda(lambda x: _rotate_image(x, angle)),\n",
    "                   Pad(28)\n",
    "                   ])\n",
    "  return transform\n",
    "\n",
    "class Pad(object):\n",
    "  def __init__(self, size, fill=0, padding_mode='constant'):\n",
    "    self.size = size\n",
    "    self.fill = fill\n",
    "    self.padding_mode = padding_mode\n",
    "    \n",
    "  def __call__(self, img):\n",
    "    # If the H and W of img is not equal to desired size,\n",
    "    # then pad the channel of img to desired size.\n",
    "    img_size = img.size()[1]\n",
    "    assert ((self.size - img_size) % 2 == 0)\n",
    "    padding = (self.size - img_size) // 2\n",
    "    padding = (padding, padding, padding, padding)\n",
    "    return F.pad(img, padding, self.padding_mode, self.fill)\n",
    "\n",
    "class Data():\n",
    "  def __init__(self, path, train=True, angle=None):\n",
    "    transform = get_transform(angle)\n",
    "    self.dataset = datasets.MNIST(root=os.path.join(path, \"MNIST\"), transform=transform, train=train, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDEOJ436rs88"
   },
   "source": [
    "### Dataloaders and Arguments\n",
    "- Training Arguments\n",
    "- Setup 5 different Rotations\n",
    "- 5 Train DataLoader\n",
    "- 5 Test DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435,
     "referenced_widgets": [
      "6678e54bfcce4aaa80134cf29d543bce",
      "d6b498ee01f74b12a51dc944e633c304",
      "e4c801c7dea848b8b392808ba94acc07",
      "954f009683e34fa2871775d8eebea8ba",
      "afe49763962c4bc293ba8692feaf8078",
      "7c9171136dcc426baaddb573f587a200",
      "74e98976de5b465a8218a4f5398d89ee",
      "3c57de7448674f718a2c12f7a8a3dc73",
      "bd3b7a2948c6460db33ecebebacb3e6e",
      "262e84a4c30448a89f9a4fcb09d718d4",
      "342d7362bdc242919b28d8d389b5ed2c",
      "5bf7bd4c4ac047b382f0d85a558704b1",
      "557c67df0db540a0b0597d1ba64fe2c7",
      "a863f04a60404ece8e3492705d56082d",
      "019c9ffd1526488887944f9d3d3ac559",
      "21bf55b0dad04eb3baa460994dfffa2e",
      "d71a16db1bd043cba776108b5b2024a5",
      "bb2e25bff7c84ab0b613b9bfa4d9a3bf",
      "6bfe7158ed774e70ab3e93c985208b02",
      "a3a8cdd6807d4c2c90af0cfb1708fefb",
      "ed01d150ab1946b9b26ebfc1028ce74c",
      "8059df5c69ad48af8627f7bae3cb1c14",
      "49683c778d6d4b2b8eef56ff767d1a5f",
      "cd204aebd36c4a51b9a3e076813c22f5",
      "a4b5d00ba1394ee3a84a6b0482392250",
      "9574e0c85a9b4fbc8d5688297e75c4de",
      "b73534bd79204873a0363689e54546c1",
      "bbf27d4324d84ea182524b00685141bc",
      "b53d43d75e34457fab24ee729e816c2b",
      "489df34a47d64f5e8069c06d26d1e9cc",
      "cb1dd9f39ad543698a883694d737d62d",
      "adb1230c890a4d099c71ef949c3933a5",
      "9e3719f2f271462e98808d4aa595e3db",
      "4c0f433640d84267ad7d429eba7822a9",
      "e508442a7aa34b358b9970ae313a26f8",
      "a52e3522bb854861a370e5ddf6f52bc2",
      "87a4cd167a794115998e6505f6dabc44",
      "8c77c242a7c1474b9e2758291c57dd44",
      "9514e4e67c7b4ac28bd4e0f513027d9f",
      "5cea3374e02547308903a5b30776911b",
      "74232ccdd40247378f4173e6bd6d306e",
      "71ecc787a84641b1ae638ca713d61ffc",
      "988647e20dbd45d9b6dfce4a7a06ffbd",
      "fead6467c43340a48571fa8d5f3d61d1"
     ]
    },
    "id": "cD2GEHlOrNQ2",
    "outputId": "8677d90b-61b0-4d15-aa8c-e4e11e583b0c"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "  task_number = 5\n",
    "  epochs_per_task = 10\n",
    "  lr = 1.0e-4\n",
    "  batch_size = 128\n",
    "  test_size=8192\n",
    "\n",
    "args=Args()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# generate rotations for the tasks.\n",
    "\n",
    "# generate rotated MNIST data from 10 different rotations.\n",
    "\n",
    "angle_list = [20 * x for x in range(args.task_number)]\n",
    "\n",
    "# prepare rotated MNIST datasets.\n",
    "\n",
    "train_datasets = [Data('data', angle=angle_list[index]) for index in range(args.task_number)]\n",
    "train_dataloaders = [DataLoader(data.dataset, batch_size=args.batch_size, shuffle=True) for data in train_datasets]\n",
    "\n",
    "test_datasets = [Data('data', train=False, angle=angle_list[index]) for index in range(args.task_number)]\n",
    "test_dataloaders = [DataLoader(data.dataset, batch_size=args.test_size, shuffle=True) for data in test_datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xBPkbzbvT_c"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "tNcH1lTKvgm9",
    "outputId": "81ecf17d-6e45-4f61-e328-1aba47663920"
   },
   "outputs": [],
   "source": [
    "# Visualize label 0-9 1 sample MNIST picture in 5 tasks.\n",
    "sample = [Data('data', angle=angle_list[index]) for index in range(args.task_number)]\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "for task in range(5):\n",
    "  target_list = []\n",
    "  cnt = 0\n",
    "  while (len(target_list) < 10):\n",
    "    img, target = sample[task].dataset[cnt]\n",
    "    cnt += 1\n",
    "    if target in target_list:\n",
    "      continue\n",
    "    else:\n",
    "      target_list.append(target)\n",
    "    plt.subplot(5, 10, (task)*10 + target + 1)\n",
    "    curr_img = np.reshape(img, (28, 28))\n",
    "    plt.matshow(curr_img, cmap=plt.get_cmap('gray'), fignum=False)\n",
    "    ax = plt.gca()\n",
    "    ax.axes.xaxis.set_ticks([])\n",
    "    ax.axes.yaxis.set_ticks([])\n",
    "    plt.title(\"task: \" + str(task+1) + \" \" + \"label: \" + str(target), y=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRh6nIcMC6vy"
   },
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLv7CHTHtf53"
   },
   "source": [
    "### Model Architecture\n",
    "To fair comparison, \n",
    "\n",
    "We fix our model architecture to do this homework. \n",
    "\n",
    "The model architecture consists of 4 layers fully-connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnpIMLMSs_3k",
    "outputId": "f658c764-1372-483b-ba90-31279d260011"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  \"\"\"\n",
    "  Model architecture \n",
    "  1*28*28 (input) → 1024 → 512 → 256 → 10\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.fc1 = nn.Linear(1*28*28, 1024)\n",
    "    self.fc2 = nn.Linear(1024, 512)\n",
    "    self.fc3 = nn.Linear(512, 256)\n",
    "    self.fc4 = nn.Linear(256, 10)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, 1*28*28)\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc3(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc4(x)\n",
    "    return x\n",
    "\n",
    "example = Model()\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABoTzmtiumsv"
   },
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPxS7H4DuQ2W"
   },
   "source": [
    "### Train\n",
    "This is our function of training.\n",
    "\n",
    "It can generally be applied in different regularization-based lifelong learning algorithms in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO6q7ymIuOTR"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader, epochs_per_task, lll_object, lll_lambda, test_dataloaders, evaluate, device, log_step=1):\n",
    "  model.train()\n",
    "  model.zero_grad()\n",
    "  objective = nn.CrossEntropyLoss()\n",
    "  acc_per_epoch = []\n",
    "  loss = 1.0\n",
    "  bar = tqdm.auto.trange(epochs_per_task, leave=False, desc=f\"Epoch 1, Loss: {loss:.7f}\")\n",
    "  for epoch in bar:\n",
    "    for imgs, labels in tqdm.auto.tqdm(dataloader, leave=False):            \n",
    "      imgs, labels = imgs.to(device), labels.to(device)\n",
    "      outputs = model(imgs)\n",
    "      loss = objective(outputs, labels)\n",
    "      total_loss = loss\n",
    "      lll_loss = lll_object.penalty(model)\n",
    "      total_loss += lll_lambda * lll_loss \n",
    "      lll_object.update(model)\n",
    "      optimizer.zero_grad()\n",
    "      total_loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss = total_loss.item()\n",
    "      bar.set_description_str(desc=f\"Epoch {epoch+1:2}, Loss: {loss:.7f}\", refresh=True)\n",
    "    acc_average  = []\n",
    "    for test_dataloader in test_dataloaders: \n",
    "      acc_test = evaluate(model, test_dataloader, device)\n",
    "      acc_average.append(acc_test)\n",
    "    average=np.mean(np.array(acc_average))\n",
    "    acc_per_epoch.append(average*100.0)\n",
    "    bar.set_description_str(desc=f\"Epoch {epoch+2:2}, Loss: {loss:.7f}\", refresh=True)\n",
    "                \n",
    "  return model, optimizer, acc_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0UF5Ef2us9G"
   },
   "source": [
    "### Evaluate\n",
    "This is our function of evaluation.\n",
    "\n",
    "It can generally be applied in different regularization-based lifelong learning algorithms in this homework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5WPNr38usXt"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    correct_cnt = 0\n",
    "    total = 0\n",
    "    for imgs, labels in test_dataloader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, pred_label = torch.max(outputs.data, 1)\n",
    "\n",
    "        correct_cnt += (pred_label == labels.data).sum().item()\n",
    "        total += torch.ones_like(labels.data).sum().item()\n",
    "    return correct_cnt / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJSpRj3BD7jF"
   },
   "source": [
    "# Methods\n",
    "- Baseline\n",
    "- EWC\n",
    "- SI\n",
    "- MAS\n",
    "- RWalk\n",
    "- SCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRDSy-keEHcb"
   },
   "source": [
    "### Baseline\n",
    "The baseline class will do nothing in the regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sQz7s_-4Olb"
   },
   "outputs": [],
   "source": [
    "# Baseline\n",
    "class baseline(object):\n",
    "  \"\"\"\n",
    "  baseline technique: do nothing in regularization term [initialize and all weight is zero]\n",
    "  \"\"\"\n",
    "  def __init__(self, model, dataloader, device):\n",
    "    self.model = model\n",
    "    self.dataloader = dataloader\n",
    "    self.device = device\n",
    "    # extract all parameters in models\n",
    "    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad} \n",
    "    \n",
    "    # store current parameters\n",
    "    self.p_old = {} \n",
    "\n",
    "    # generate weight matrix\n",
    "    self._precision_matrices = self._calculate_importance()  \n",
    "\n",
    "    for n, p in self.params.items():\n",
    "      # keep the old parameter in self.p_old\n",
    "      self.p_old[n] = p.clone().detach() \n",
    "\n",
    "  def _calculate_importance(self):\n",
    "    precision_matrices = {} \n",
    "    # initialize weight matrix（fill zero）\n",
    "    for n, p in self.params.items(): \n",
    "      precision_matrices[n] = p.clone().detach().fill_(0)\n",
    "\n",
    "    return precision_matrices\n",
    "\n",
    "  def penalty(self, model: nn.Module):\n",
    "    loss = 0\n",
    "    for n, p in model.named_parameters():\n",
    "      _loss = self._precision_matrices[n] * (p - self.p_old[n]) ** 2\n",
    "      loss += _loss.sum()\n",
    "    return loss\n",
    "  \n",
    "  def update(self, model):\n",
    "    # do nothing\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jq_AudHg4YoW"
   },
   "outputs": [],
   "source": [
    "# Baseline\n",
    "print(\"RUN BASELINE\")\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# initialize lifelong learning object (baseline class) without adding any regularization term.\n",
    "lll_object=baseline(model=model, dataloader=None, device=device)\n",
    "lll_lambda=0.0\n",
    "baseline_acc=[]\n",
    "task_bar = tqdm.auto.trange(len(train_dataloaders),desc=\"Task   1\")\n",
    "\n",
    "# iterate training on each task continually.\n",
    "for train_indexes in task_bar:\n",
    "  # Train each task\n",
    "  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, \n",
    "                  lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n",
    "  \n",
    "  # get model weight to baseline class and do nothing!\n",
    "  lll_object=baseline(model=model, dataloader=train_dataloaders[train_indexes],device=device)\n",
    "  \n",
    "  # new a optimizer\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "  \n",
    "  # Collect average accuracy in each epoch\n",
    "  baseline_acc.extend(acc_list)\n",
    "  \n",
    "  # display the information of the next task.\n",
    "  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n",
    "\n",
    "# average accuracy in each task per epoch! \n",
    "print(baseline_acc)\n",
    "print(\"==================================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIZ2HiZwELR3"
   },
   "source": [
    "### EWC\n",
    "\n",
    "Elastic Weight Consolidation\n",
    "\n",
    "The ewc class applied EWC algorithm to calculate the regularization term. The central concept is included in Prof.Hung-yi's lectures. Here we will focus on the algorithm of EWC.\n",
    "\n",
    "In this assignment, we want to let our model learn 10 tasks successively. Here we show a simple example that lets the model learn 2 tasks(task A and task B) successively.\n",
    "\n",
    "In the EWC algorithm, the definition of the loss function is shown below:\n",
    " $$\\mathcal{L}_B = \\mathcal{L}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_i (\\theta_{i} - \\theta_{A,i}^{*})^2  $$\n",
    "  \n",
    "Assume we have a neural network with more than two parameters.\n",
    "\n",
    "$F_i$ corresponds to the $i^{th}$ guard in Prof. Hung-yi's lecture. Please do not modify this parameter, because it's important to task A.\n",
    "\n",
    "The definition of $F$ is shown below.\n",
    "$$ F = [ \\nabla \\log(p(y_n | x_n, \\theta_{A}^{*})) \\nabla \\log(p(y_n | x_n, \\theta_{A}^{*}))^T ] $$ \n",
    "\n",
    "We only take the diagonal value of the matrix to approximate each parameter's $F_i$.\n",
    "\n",
    "The detail information and derivation are shown in 2.4.1 and 2.4 of [Continual Learning in Neural\n",
    "Networks](https://arxiv.org/pdf/1910.02718.pdf)\n",
    "\n",
    "For You Information: [Elastic Weight Consolidation](https://arxiv.org/pdf/1612.00796.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cvqQKL1EVss"
   },
   "outputs": [],
   "source": [
    "# EWC\n",
    "class ewc(object):\n",
    "  \"\"\"\n",
    "  @article{kirkpatrick2017overcoming,\n",
    "      title={Overcoming catastrophic forgetting in neural networks},\n",
    "      author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},\n",
    "      journal={Proceedings of the national academy of sciences},\n",
    "      year={2017},\n",
    "      url={https://arxiv.org/abs/1612.00796}\n",
    "  }\n",
    "  \"\"\"\n",
    "  def __init__(self, model, dataloader, device, prev_guards=[None]):\n",
    "    self.model = model\n",
    "    self.dataloader = dataloader\n",
    "    self.device = device\n",
    "    # extract all parameters in models\n",
    "    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad} \n",
    "    \n",
    "    # initialize parameters\n",
    "    self.p_old = {}\n",
    "    # save previous guards\n",
    "    self.previous_guards_list = prev_guards\n",
    "\n",
    "    # generate Fisher (F) matrix for EWC\n",
    "    self._precision_matrices = self._calculate_importance()                   \n",
    "\n",
    "    # keep the old parameter in self.p_old\n",
    "    for n, p in self.params.items():\n",
    "      self.p_old[n] = p.clone().detach()       \n",
    "\n",
    "  def _calculate_importance(self):\n",
    "    precision_matrices = {}\n",
    "    # initialize Fisher (F) matrix（all fill zero）and add previous guards\n",
    "    for n, p in self.params.items(): \n",
    "      precision_matrices[n] = p.clone().detach().fill_(0)                 \n",
    "      for i in range(len(self.previous_guards_list)):\n",
    "        if self.previous_guards_list[i]:\n",
    "          precision_matrices[n] += self.previous_guards_list[i][n]\n",
    "\n",
    "    self.model.eval()\n",
    "    if self.dataloader is not None:\n",
    "      number_data = len(self.dataloader)\n",
    "      for data in self.dataloader:\n",
    "        self.model.zero_grad()\n",
    "        # get image data\n",
    "        input = data[0].to(self.device)\n",
    "          \n",
    "        # image data forward model\n",
    "        output = self.model(input)\n",
    "          \n",
    "        # Simply use groud truth label of dataset.  \n",
    "        label = data[1].to(self.device)\n",
    "          \n",
    "        # generate Fisher(F) matrix for EWC     \n",
    "        loss = F.nll_loss(F.log_softmax(output, dim=1), label)\n",
    "        loss.backward()   \n",
    "\n",
    "        for n, p in self.model.named_parameters():\n",
    "          # get the gradient of each parameter and square it, then average it in all validation set.                          \n",
    "          precision_matrices[n].data += p.grad.data ** 2 / number_data   \n",
    "                                                                \n",
    "      precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "    return precision_matrices\n",
    "\n",
    "  def penalty(self, model: nn.Module):\n",
    "    loss = 0\n",
    "    for n, p in model.named_parameters():\n",
    "      # generate the final regularization term by the ewc weight (self._precision_matrices[n]) and the square of weight difference ((p - self.p_old[n]) ** 2).  \n",
    "      _loss = self._precision_matrices[n] * (p - self.p_old[n]) ** 2\n",
    "      loss += _loss.sum()\n",
    "    return loss\n",
    "  \n",
    "  def update(self, model):\n",
    "    # do nothing\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VJ9QUsNIyEG"
   },
   "outputs": [],
   "source": [
    "# EWC\n",
    "print(\"RUN EWC\")\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# initialize lifelong learning object for EWC\n",
    "lll_object=ewc(model=model, dataloader=None, device=device)\n",
    "\n",
    "# setup the coefficient value of regularization term.\n",
    "lll_lambda=100\n",
    "ewc_acc= []\n",
    "task_bar = tqdm.auto.trange(len(train_dataloaders),desc=\"Task   1\")\n",
    "prev_guards = []\n",
    "\n",
    "# iterate training on each task continually.\n",
    "for train_indexes in task_bar:\n",
    "  # Train Each Task\n",
    "  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n",
    "  \n",
    "  # get model weight and calculate guidance for each weight\n",
    "  prev_guards.append(lll_object._precision_matrices)\n",
    "  lll_object=ewc(model=model, dataloader=train_dataloaders[train_indexes], device=device, prev_guards=prev_guards)\n",
    "\n",
    "  # new a Optimizer\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "  # collect average accuracy in each epoch\n",
    "  ewc_acc.extend(acc_list)\n",
    "\n",
    "  # Update tqdm displayer\n",
    "  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n",
    "\n",
    "# average accuracy in each task per epoch!     \n",
    "print(ewc_acc)\n",
    "print(\"==================================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVcXQ2ztJHIq"
   },
   "source": [
    "### MAS\n",
    "Memory Aware Synapses\n",
    "\n",
    "The mas class applied MAS algorithm to calculate the regularization term.\n",
    "\n",
    "The concept of MAS is similar to EWC, the only difference is the calculation of the important weight. \n",
    "The details are mentioned in the following blocks.\n",
    "\n",
    "MAS:\n",
    "\n",
    "In MAS, the Loss function is shown below, the model learns task A before it learned task B.\n",
    "\n",
    "$$\\mathcal{L}_B = \\mathcal{L}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} \\Omega_i (\\theta_{i} - \\theta_{A,i}^{*})^2$$\n",
    "\n",
    "Compare with EWC, the $F_i$ in the loss function is replaced with $\\Omega_i$ in the following function.\n",
    "\n",
    "$$\\Omega_i = || \\frac{\\partial \\ell_2^2(M(x_k; \\theta))}{\\partial \\theta_i} || $$ \n",
    "\n",
    "$x_k$ is the sample data of the previous task. So the $\\Omega$ is obtained gradients of the squared L2-norm of the learned network output.\n",
    "\n",
    "The method proposed in the paper is the local version by taking squared L2-norm outputs from each layer of the model.\n",
    "\n",
    "Here we only want you to implement the global version by taking outputs from the last layer of the model.\n",
    "\n",
    "For Your Information: \n",
    "[Memory Aware Synapses](https://arxiv.org/pdf/1711.09601.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lKFEzSLJNBX"
   },
   "outputs": [],
   "source": [
    "class mas(object):\n",
    "  \"\"\"\n",
    "  @article{aljundi2017memory,\n",
    "      title={Memory Aware Synapses: Learning what (not) to forget},\n",
    "      author={Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},\n",
    "      booktitle={ECCV},\n",
    "      year={2018},\n",
    "      url={https://eccv2018.org/openaccess/content_ECCV_2018/papers/Rahaf_Aljundi_Memory_Aware_Synapses_ECCV_2018_paper.pdf}\n",
    "  }\n",
    "  \"\"\"\n",
    "  def __init__(self, model: nn.Module, dataloader, device, prev_guards=[None]):\n",
    "    self.model = model \n",
    "    self.dataloader = dataloader\n",
    "    # extract all parameters in models\n",
    "    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad} \n",
    "    \n",
    "    # initialize parameters\n",
    "    self.p_old = {} \n",
    "    \n",
    "    self.device = device\n",
    "\n",
    "    # save previous guards\n",
    "    self.previous_guards_list = prev_guards\n",
    "    \n",
    "    # generate Omega(Ω) matrix for MAS\n",
    "    self._precision_matrices = self.calculate_importance() \n",
    "\n",
    "    # keep the old parameter in self.p_old\n",
    "    for n, p in self.params.items():\n",
    "      self.p_old[n] = p.clone().detach() \n",
    "  \n",
    "  def calculate_importance(self):\n",
    "    precision_matrices = {}\n",
    "    # initialize Omega(Ω) matrix（all filled zero）\n",
    "    for n, p in self.params.items():\n",
    "      precision_matrices[n] = p.clone().detach().fill_(0) \n",
    "      for i in range(len(self.previous_guards_list)):\n",
    "        if self.previous_guards_list[i]:\n",
    "          precision_matrices[n] += self.previous_guards_list[i][n]\n",
    "\n",
    "    self.model.eval()\n",
    "    if self.dataloader is not None:\n",
    "      num_data = len(self.dataloader)\n",
    "      for data in self.dataloader:\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(data[0].to(self.device))\n",
    "        ################################################################\n",
    "        #####  TODO: generate Omega(Ω) matrix for MAS.  #####   \n",
    "        ################################################################\n",
    "        pass   \n",
    "        ################################################################                  \n",
    "    \n",
    "      precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "    return precision_matrices\n",
    "\n",
    "  def penalty(self, model: nn.Module):\n",
    "    loss = 0\n",
    "    for n, p in model.named_parameters():\n",
    "      _loss = self._precision_matrices[n] * (p - self.p_old[n]) ** 2\n",
    "      loss += _loss.sum()\n",
    "    return loss\n",
    "\n",
    "  def update(self, model):\n",
    "    # do nothing\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wn1VGt38KSG-"
   },
   "outputs": [],
   "source": [
    "# MAS\n",
    "print(\"RUN MAS\")\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "lll_object=mas(model=model, dataloader=None, device=device)\n",
    "lll_lambda=0.1\n",
    "mas_acc= []\n",
    "task_bar = tqdm.auto.trange(len(train_dataloaders),desc=\"Task   1\")\n",
    "prev_guards = []\n",
    "\n",
    "for train_indexes in task_bar:\n",
    "  # Train Each Task\n",
    "  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n",
    "  \n",
    "  # get model weight and calculate guidance for each weight\n",
    "  prev_guards.append(lll_object._precision_matrices)\n",
    "  lll_object=mas(model=model, dataloader=train_dataloaders[train_indexes], device=device, prev_guards=prev_guards)\n",
    "\n",
    "  # New a Optimizer\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "  # Collect average accuracy in each epoch\n",
    "  mas_acc.extend(acc_list)\n",
    "  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n",
    "\n",
    "# average accuracy in each task per epoch!     \n",
    "print(mas_acc)\n",
    "print(\"==================================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI-iLmEZKoWH"
   },
   "source": [
    "### SI\n",
    "The si class applied SI (Synaptic Intelligence) algorithm to calculate the regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rXpQIv8Kvsb"
   },
   "outputs": [],
   "source": [
    "# SI\n",
    "class si(object):\n",
    "  \"\"\"\n",
    "  @article{kirkpatrick2017overcoming,\n",
    "      title={Overcoming catastrophic forgetting in neural networks},\n",
    "      author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},\n",
    "      journal={Proceedings of the national academy of sciences},\n",
    "      year={2017},\n",
    "      url={https://arxiv.org/abs/1612.00796}\n",
    "  }\n",
    "  \"\"\"\n",
    "  def __init__(self, model, dataloader, epsilon, device):\n",
    "    self.model = model\n",
    "    self.dataloader = dataloader\n",
    "    self.device = device\n",
    "    self.epsilon = epsilon\n",
    "    # extract all parameters in models\n",
    "    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "    \n",
    "    self._n_p_prev, self._n_omega = self._calculate_importance() \n",
    "    self.W, self.p_old = self._init_()\n",
    "    \n",
    "\n",
    "  def _init_(self):\n",
    "    W = {}\n",
    "    p_old = {}\n",
    "    for n, p in self.model.named_parameters():\n",
    "      n = n.replace('.', '__')\n",
    "      if p.requires_grad:\n",
    "        W[n] = p.data.clone().zero_()\n",
    "        p_old[n] = p.data.clone()\n",
    "    return W, p_old\n",
    "\n",
    "  def _calculate_importance(self):\n",
    "    n_p_prev = {}\n",
    "    n_omega = {}\n",
    "\n",
    "    if self.dataloader != None:\n",
    "      for n, p in self.model.named_parameters():\n",
    "        n = n.replace('.', '__')\n",
    "        if p.requires_grad:\n",
    "          # Find/calculate new values for quadratic penalty on parameters\n",
    "          p_prev = getattr(self.model, '{}_SI_prev_task'.format(n))\n",
    "          W = getattr(self.model, '{}_W'.format(n))\n",
    "          p_current = p.detach().clone()\n",
    "          p_change = p_current - p_prev\n",
    "          omega_add = W/(p_change**2 + self.epsilon)\n",
    "          try:\n",
    "            omega = getattr(self.model, '{}_SI_omega'.format(n))\n",
    "          except AttributeError:\n",
    "            omega = p.detach().clone().zero_()\n",
    "          omega_new = omega + omega_add\n",
    "          n_omega[n] = omega_new\n",
    "          n_p_prev[n] = p_current\n",
    "\n",
    "          # Store these new values in the model\n",
    "          self.model.register_buffer('{}_SI_prev_task'.format(n), p_current)\n",
    "          self.model.register_buffer('{}_SI_omega'.format(n), omega_new)\n",
    "\n",
    "    else:\n",
    "      for n, p in self.model.named_parameters():\n",
    "        n = n.replace('.', '__')\n",
    "        if p.requires_grad:\n",
    "          n_p_prev[n] = p.detach().clone()\n",
    "          n_omega[n] = p.detach().clone().zero_()\n",
    "          self.model.register_buffer('{}_SI_prev_task'.format(n), p.detach().clone())\n",
    "    return n_p_prev, n_omega\n",
    "\n",
    "  def penalty(self, model: nn.Module):\n",
    "    loss = 0.0\n",
    "    for n, p in model.named_parameters():\n",
    "      n = n.replace('.', '__')\n",
    "      if p.requires_grad:\n",
    "        prev_values = self._n_p_prev[n]\n",
    "        omega = self._n_omega[n]\n",
    "        _loss = omega * (p - prev_values) ** 2\n",
    "        loss += _loss.sum()\n",
    "    return loss\n",
    "  \n",
    "  def update(self, model):\n",
    "    for n, p in model.named_parameters():\n",
    "      n = n.replace('.', '__')\n",
    "      if p.requires_grad:\n",
    "        if p.grad is not None:\n",
    "          self.W[n].add_(-p.grad * (p.detach() - self.p_old[n]))\n",
    "          self.model.register_buffer('{}_W'.format(n), self.W[n])\n",
    "        self.p_old[n] = p.detach().clone()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OH6OsfOU8Ac"
   },
   "outputs": [],
   "source": [
    "# SI\n",
    "print(\"RUN SI\")\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "lll_object=si(model=model, dataloader=None, epsilon=0.1, device=device)\n",
    "lll_lambda=1\n",
    "si_acc = []\n",
    "task_bar = tqdm.auto.trange(len(train_dataloaders),desc=\"Task   1\")\n",
    "\n",
    "for train_indexes in task_bar:\n",
    "  # Train Each Task\n",
    "  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n",
    "  \n",
    "  # get model weight and calculate guidance for each weight\n",
    "  lll_object=si(model=model, dataloader=train_dataloaders[train_indexes], epsilon=0.1, device=device)\n",
    "\n",
    "  # New a Optimizer\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "  # Collect average accuracy in each epoch\n",
    "  si_acc.extend(acc_list)\n",
    "  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n",
    "\n",
    "# average accuracy in each task per epoch!     \n",
    "print(si_acc)\n",
    "print(\"==================================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6Y661vxVOHQ"
   },
   "source": [
    "### RWalk\n",
    "\n",
    "#### Remanian Walk for Incremental Learning\n",
    "\n",
    "The rwalk class applied Remanian Walk algorithm to calculate the regularization term.\n",
    "\n",
    "The details are mentioned in the following blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUFOqSFeVYwT"
   },
   "outputs": [],
   "source": [
    "class rwalk(object):\n",
    "  def __init__(self, model, dataloader, epsilon, device, prev_guards=[None]):\n",
    "    self.model = model\n",
    "    self.dataloader = dataloader\n",
    "    self.device = device\n",
    "    self.epsilon = epsilon\n",
    "    self.update_ewc_parameter = 0.4\n",
    "    # extract model parameters and store in dictionary\n",
    "    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "    \n",
    "    # initialize the guidance matrix\n",
    "    self._means = {} \n",
    "\n",
    "    self.previous_guards_list = prev_guards\n",
    "    \n",
    "    # Generate Fisher (F) Information Matrix\n",
    "    self._precision_matrices = self._calculate_importance_ewc()\n",
    "      \n",
    "    self._n_p_prev, self._n_omega = self._calculate_importance() \n",
    "    self.W, self.p_old = self._init_()\n",
    "\n",
    "  def _init_(self):\n",
    "    W = {}\n",
    "    p_old = {}\n",
    "    for n, p in self.model.named_parameters():\n",
    "      n = n.replace('.', '__')\n",
    "      if p.requires_grad:\n",
    "        W[n] = p.data.clone().zero_()\n",
    "        p_old[n] = p.data.clone()\n",
    "    return W, p_old\n",
    "\n",
    "  def _calculate_importance(self):\n",
    "    n_p_prev = {}\n",
    "    n_omega = {}\n",
    "\n",
    "    if self.dataloader is not None:\n",
    "      for n, p in self.model.named_parameters():\n",
    "        n = n.replace('.', '__')\n",
    "        if p.requires_grad:\n",
    "          # Find/calculate new values for quadratic penalty on parameters\n",
    "          p_prev = getattr(self.model, '{}_SI_prev_task'.format(n))\n",
    "          W = getattr(self.model, '{}_W'.format(n))\n",
    "          p_current = p.detach().clone()\n",
    "          p_change = p_current - p_prev\n",
    "          omega_add = W / (1.0 / 2.0*self._precision_matrices[n] *p_change**2 + self.epsilon)\n",
    "          try:\n",
    "              omega = getattr(self.model, '{}_SI_omega'.format(n))\n",
    "          except AttributeError:\n",
    "              omega = p.detach().clone().zero_()\n",
    "          omega_new = 0.5 * omega + 0.5 *omega_add\n",
    "          n_omega[n] = omega_new\n",
    "          n_p_prev[n] = p_current\n",
    "\n",
    "          # Store these new values in the model\n",
    "          self.model.register_buffer('{}_SI_prev_task'.format(n), p_current)\n",
    "          self.model.register_buffer('{}_SI_omega'.format(n), omega_new)\n",
    "\n",
    "    else:\n",
    "      for n, p in self.model.named_parameters():\n",
    "        n = n.replace('.', '__')\n",
    "        if p.requires_grad:\n",
    "          n_p_prev[n] = p.detach().clone()\n",
    "          n_omega[n] = p.detach().clone().zero_()\n",
    "          self.model.register_buffer('{}_SI_prev_task'.format(n), p.detach().clone())\n",
    "    return n_p_prev, n_omega\n",
    "  \n",
    "  def _calculate_importance_ewc(self):\n",
    "    precision_matrices = {}\n",
    "    for n, p in self.params.items():\n",
    "      # initialize Fisher (F) matrix（all fill zero） \n",
    "      n = n.replace('.', '__') \n",
    "      precision_matrices[n] = p.clone().detach().fill_(0)\n",
    "      for i in range(len(self.previous_guards_list)):\n",
    "        if self.previous_guards_list[i]:\n",
    "          precision_matrices[n] += self.previous_guards_list[i][n]\n",
    "           \n",
    "\n",
    "    self.model.eval()\n",
    "    if self.dataloader is not None:\n",
    "      number_data = len(self.dataloader)\n",
    "      for n, p in self.model.named_parameters():                         \n",
    "        n = n.replace('.', '__')\n",
    "        precision_matrices[n].data *= (1 - self.update_ewc_parameter)\n",
    "      for data in self.dataloader:\n",
    "        self.model.zero_grad()\n",
    "        input = data[0].to(self.device)\n",
    "        output = self.model(input)\n",
    "        label = data[1].to(self.device)\n",
    "\n",
    "        # generate Fisher(F) matrix for RWALK    \n",
    "        loss = F.nll_loss(F.log_softmax(output, dim=1), label) \n",
    "        loss.backward()                                                    \n",
    "                                                                          \n",
    "        for n, p in self.model.named_parameters():                         \n",
    "          n = n.replace('.', '__')\n",
    "          precision_matrices[n].data += self.update_ewc_parameter*p.grad.data ** 2 / number_data  \n",
    "                                                                  \n",
    "      precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "    return precision_matrices\n",
    "\n",
    "  def penalty(self, model: nn.Module):\n",
    "    loss = 0.0\n",
    "    for n, p in model.named_parameters():\n",
    "      n = n.replace('.', '__')\n",
    "      if p.requires_grad:\n",
    "        prev_values = self._n_p_prev[n]\n",
    "        omega = self._n_omega[n]\n",
    "        # Generate regularization term  _loss by omega and Fisher Matrix\n",
    "        _loss = (omega + self._precision_matrices[n]) * (p - prev_values) ** 2\n",
    "        loss += _loss.sum()\n",
    "\n",
    "    return loss\n",
    "  \n",
    "  def update(self, model):\n",
    "    for n, p in model.named_parameters():\n",
    "      n = n.replace('.', '__')\n",
    "      if p.requires_grad:\n",
    "        if p.grad is not None:\n",
    "          self.W[n].add_(-p.grad * (p.detach() - self.p_old[n]))\n",
    "          self.model.register_buffer('{}_W'.format(n), self.W[n])\n",
    "        self.p_old[n] = p.detach().clone()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6wH-Nc2XgJY"
   },
   "outputs": [],
   "source": [
    "# RWalk\n",
    "print(\"RUN Rwalk\")\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "lll_object=rwalk(model=model, dataloader=None, epsilon=0.1, device=device)\n",
    "lll_lambda=100\n",
    "rwalk_acc = []\n",
    "task_bar = tqdm.auto.trange(len(train_dataloaders),desc=\"Task   1\")\n",
    "prev_guards = []\n",
    "\n",
    "for train_indexes in task_bar:\n",
    "  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n",
    "  prev_guards.append(lll_object._precision_matrices)\n",
    "  lll_object=rwalk(model=model, dataloader=train_dataloaders[train_indexes], epsilon=0.1, device=device, prev_guards=prev_guards)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "  rwalk_acc.extend(acc_list)\n",
    "  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n",
    "\n",
    "# average accuracy in each task per epoch!     \n",
    "print(rwalk_acc)\n",
    "print(\"==================================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0mm4MhmXnti"
   },
   "source": [
    "### SCP\n",
    "Sliced Cramer Preservation\n",
    "\n",
    "Pseudo Code:\n",
    "<img src=\"https://i.ibb.co/QJycmNZ/2021-02-18-21-07.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71W75eFhXvrB"
   },
   "outputs": [],
   "source": [
    "def sample_spherical(npoints, ndim=3):\n",
    "  vec = np.random.randn(ndim, npoints)\n",
    "  vec /= np.linalg.norm(vec, axis=0)\n",
    "  return torch.from_numpy(vec)\n",
    "\n",
    "class scp(object):\n",
    "  \"\"\"\n",
    "  OPEN REVIEW VERSION:\n",
    "  https://openreview.net/forum?id=BJge3TNKwH\n",
    "  \"\"\"\n",
    "  def __init__(self, model: nn.Module, dataloader, L: int, device, prev_guards=[None]):\n",
    "    self.model = model \n",
    "    self.dataloader = dataloader\n",
    "    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "    self._state_parameters = {}\n",
    "    self.L= L\n",
    "    self.device = device\n",
    "    self.previous_guards_list = prev_guards\n",
    "    self._precision_matrices = self.calculate_importance()\n",
    "    for n, p in self.params.items():\n",
    "      self._state_parameters[n] = p.clone().detach()\n",
    "  \n",
    "  def calculate_importance(self):\n",
    "    precision_matrices = {}\n",
    "    for n, p in self.params.items():\n",
    "      precision_matrices[n] = p.clone().detach().fill_(0)\n",
    "      for i in range(len(self.previous_guards_list)):\n",
    "        if self.previous_guards_list[i]:\n",
    "          precision_matrices[n] += self.previous_guards_list[i][n]\n",
    "\n",
    "    self.model.eval()\n",
    "    if self.dataloader is not None:\n",
    "      num_data = len(self.dataloader)\n",
    "      for data in self.dataloader:\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(data[0].to(self.device))\n",
    "          \n",
    "        mean_vec = output.mean(dim=0)\n",
    "\n",
    "        L_vectors = sample_spherical(self.L, output.shape[-1])\n",
    "        L_vectors = L_vectors.transpose(1,0).to(self.device).float()\n",
    "                    \n",
    "        total_scalar = 0\n",
    "        for vec in L_vectors:\n",
    "          scalar=torch.matmul(vec, mean_vec)\n",
    "          total_scalar += scalar\n",
    "        total_scalar /= L_vectors.shape[0] \n",
    "        total_scalar.backward()     \n",
    "\n",
    "        for n, p in self.model.named_parameters():                      \n",
    "          precision_matrices[n].data += p.grad**2 / num_data      \n",
    "              \n",
    "    precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "    return precision_matrices\n",
    "\n",
    "  def penalty(self, model: nn.Module):\n",
    "    loss = 0\n",
    "    for n, p in model.named_parameters():\n",
    "      _loss = self._precision_matrices[n] * (p - self._state_parameters[n]) ** 2\n",
    "      loss += _loss.sum()\n",
    "    return loss\n",
    "  \n",
    "  def update(self, model):\n",
    "    # do nothing\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6svVnP3aiIG"
   },
   "outputs": [],
   "source": [
    "# SCP\n",
    "print(\"RUN SLICE CRAMER PRESERVATION\")\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "lll_object=scp(model=model, dataloader=None, L=100, device=device)\n",
    "lll_lambda=100\n",
    "scp_acc= []\n",
    "task_bar = tqdm.auto.trange(len(train_dataloaders),desc=\"Task   1\")\n",
    "prev_guards = []\n",
    "\n",
    "for train_indexes in task_bar:\n",
    "  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n",
    "  prev_guards.append(lll_object._precision_matrices)\n",
    "  lll_object=scp(model=model, dataloader=train_dataloaders[train_indexes], L=100, device=device, prev_guards=prev_guards)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "  scp_acc.extend(acc_list)\n",
    "  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n",
    "\n",
    "# average accuracy in each task per epoch!     \n",
    "print(scp_acc)\n",
    "print(\"==================================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNMGpoyhq4QQ"
   },
   "source": [
    "# Plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oiXbZgxq59j"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_acc(acc_list, label_list):\n",
    "  for acc, label in zip(acc_list, label_list):\n",
    "    plt.plot(acc, marker='o', lineStyle='--', linewidth=2, markersize=4, label=label)\n",
    "    plt.legend()\n",
    "  plt.savefig('acc_summary.png')\n",
    "  plt.show() \n",
    "\n",
    "acc_list = [baseline_acc, ewc_acc, mas_acc, si_acc, rwalk_acc, scp_acc]\n",
    "label_list = ['baseline', 'EWC', 'MAS', 'SI', 'RWALK', 'SCP']\n",
    "draw_acc(acc_list, label_list)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML2022Spring - HW14.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
